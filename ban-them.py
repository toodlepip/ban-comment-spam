#!/usr/bin/env python
# encoding: utf-8
'''
 -- Ban Forum Spammers

 Python script to run through an Apache log file, pull out IP addresse and check
 them again stopforumspam.com website to see if they're bad. Also check to see
 if they've left a stupidly large number of comments. If so, they get banned.
 
 The aim is to block IPs that are sending commment spam at the server level so
 the minimum number of processing power is wasted in dealing with them.

@author:     Sam Michel
            
@copyright:  2013 Sam Michel. Some rights reserved.
            
@license:    GPL v2 License

@contact:    sam@toodlepip.co.uk
'''

import sys
import os
import time
import re
import urllib
import urllib2
import json
import subprocess
import zipfile
import sqlite3 as lite
from apachelog import apachelog
from collections import defaultdict
from pprint import pprint
from optparse import OptionParser

__all__ = []
__version__ = 0.1
__date__ = '2013-11-24'
__updated__ = '2013-11-24'

DEBUG = 1
TESTRUN = 0
PROFILE = 0

REFRESH_FILE = 23*60*60
SFS_LOG_URL = "http://www.stopforumspam.com/downloads/listed_ip_90.zip"
SFS_FILE = "listed_ip_90"

'''

Pop these into the local_settings.py file, better security and easier for
testing.

HIT_THRESHOLD = 25
LOGFILE = '/path/to/apache/logfile.log'
# IPs to ignore as they might have weird use cases e.g. Home IP, Office IP,
# search engines that you'd want on the site (although these shouldn't appear
# on the stopforumspam.com lists
EXCLUDE_IPS = [
               # Office IP
               # Home IP
               ]

SFS_API_KEY = 'XXX' # Not required in this version

'''

# Pull in local configuration from local_settings.py
try:
    from local_settings import *
except:
    pass

# Copied from
# http://stackoverflow.com/questions/7806563/how-to-unzip-a-file-with-python-2-4
def unzip(path):
    zfile = zipfile.ZipFile(path)
    for name in zfile.namelist():
        (dirname, filename) = os.path.split(name)
        if filename == '':
            # directory
            if not os.path.exists(dirname):
                os.mkdir(dirname)
        else:
            # file
            fd = open(name, 'w')
            fd.write(zfile.read(name))
            fd.close()
    zfile.close()

def main(argv=None):
    '''Command line options.'''
    
    program_name = os.path.basename(sys.argv[0])
    program_version = "v%s" % __version__
    program_build_date = "%s" % __updated__
 
    program_version_string = '%%prog %s (%s)' % (program_version, program_build_date)
    #program_usage = '''usage: spam two eggs''' # optional - will be autogenerated by optparse
    program_longdesc = '''''' # optional - give further explanation about what the program does
    program_license = "Copyright 2013 user_name (organization_name)                                            \
                Licensed under the Apache License 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0"
 
    if argv is None:
        argv = sys.argv[1:]
    try:
        # setup option parser
        parser = OptionParser(version=program_version_string, epilog=program_longdesc, description=program_license)
        parser.add_option("-i", "--in", dest="infile", help="set input path [default: %default]", metavar="FILE")
        parser.add_option("-o", "--out", dest="outfile", help="set output path [default: %default]", metavar="FILE")
        parser.add_option("-v", "--verbose", dest="verbose", action="count", help="set verbosity level [default: %default]")
        parser.add_option("-r", "--run", dest="run", action="store_true", help="process the apache log and ban spammers")
        
        # set defaults
        parser.set_defaults(outfile="./out.txt", infile="./in.txt")
        
        # process options
        (opts, args) = parser.parse_args(argv)
        
        if opts.verbose > 0:
            print("verbosity level = %d" % opts.verbose)
        #if opts.infile:
            #print("infile = %s" % opts.infile)
        #if opts.outfile:
            #print("outfile = %s" % opts.outfile)
            
        # MAIN BODY #
        
        #1. Parse the apache log to see if get IPs which have posted comments
        #   and those which have looked at loads of pages
        
        if opts.run:
            
#             # Check latest file from SFS to see if its out of date. If so,
#             # set download flag so we go and grab it
#             download = False
#             try:
#                 (mode, ino, dev, nlink, uid, gid, size, atime, mtime, ctime) = \
#                     os.stat(SFS_FILE+".txt")
#                 dprint("%s last modified: %s\n", (SFS_FILE, time.ctime(mtime)))
#                 if time.time() > mtime+REFRESH_FILE:
#                     download = True
#             except:
#                 download = True
#                 pass
#             
#             # Download the latest version of file from SFS
#             # http://www.stopforumspam.com/downloads/listed_ip_90.zip
#             if download:
#                 dprint("Time to refresh file: %s", SFS_FILE)
#                 try:
#                     urllib.urlretrieve(SFS_LOG_URL, SFS_FILE+".zip")
#                     unzip(SFS_FILE+".zip")
#                 except Exception, e:
#                     print e
#                     sys.exit(-1)
#             else:
#                 dprint("%s still current", SFS_FILE)
# 
#             # Run through downloaded file and add to ipset blacklist
#             with open(SFS_FILE+".txt") as f:
#                 for i, l in enumerate(f):
#                     print "%s: %s" % (i, l)
#             
#             sys.exit()
            
            
            
            ips_to_check = parse_apache_log(file=LOGFILE, \
                threshold=HIT_THRESHOLD, exclude=EXCLUDE_IPS)
            
        #2. Run those bad IPs past http://stopforumspam.com who maintain a 
        #   good list of Forum spammers. No API key is required, but there is
        #   a cap on how many calls can be made. Suggest running this script
        #   once/day. More info: http://www.stopforumspam.com/usage

            blacklist = []
            blacklist = check_sfs(ips_to_check)
        
        #3. Generate a list of IP addresses that need to be blocked from the
        #   server and add them to the ipset blacklist.
        #   TODO: Needs a check here if the list > 65536 IPs which is the
        #   limit for an iphash using ipset
        
        # Delete all entries from blacklist
        # ipset --flush blacklist
            #subprocess.check_call(["ipset", "--flush blacklist"])
        
        # Add IPs to new blacklist set
        # ipset --add blacklist IP
            for ip in blacklist:
                print "Block: %s" % ip
                #subprocess.check_call(["ipset", "--add blacklist", ip])
        
        
    except Exception, e:
        indent = len(program_name) * " "
        sys.stderr.write(program_name + ": " + repr(e) + "\n")
        sys.stderr.write(indent + "  for help use --help")
        return 2
    
def dprint(text="", data=[]):
    if DEBUG:
        sys.stdout.write(text % data)
    
def  parse_apache_log(file=LOGFILE, threshold=HIT_THRESHOLD, \
                         exclude=EXCLUDE_IPS):
    ips = defaultdict(int)
    comments = defaultdict(int)
        
    dprint("\n\nProcessing apache file\n\n")
    format = r'%h %v %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" \"%{Cookie}i\"'
    p = apachelog.parser(format)
    dprint("Parsing apache log file: %s\n",LOGFILE)
    count = 0
    for line in open(LOGFILE):
        try:
            data = p.parse(line)
            ips[data['%h']] += 1
            if (re.search('POST /comment/reply', data['%r'])):
                comments[data['%h']] += 1
            count += 1
        except:
            sys.stderr.write("Unable to parse %s\n" % line)
    dprint("Parsed %s lines from %s\n", (count, LOGFILE))
    dprint("%s IP addresses posted %s comments\n", (len(comments), sum(comments.values())))
                    
    visitor_ips_to_check = []
    for ip in sorted(ips, key=ips.get, reverse=True):
        if ips[ip] < HIT_THRESHOLD:
            break
        if ip in EXCLUDE_IPS:
            continue
        visitor_ips_to_check.append(ip)
    dprint("%s IP addresses visited over %s times\n", \
                     (len(visitor_ips_to_check), HIT_THRESHOLD))
            
    # Merge list so there's no duplication in checking IP addresses
    ips_to_check = list(set(visitor_ips_to_check + comments.keys()))
    dprint("%s IP addresses to check (de-duped)\n", len(ips_to_check))
    return ips_to_check

def check_sfs(ips):
    _to_blacklist = []
    for i in xrange(0, len(ips), 15):
        ip_to_check = [ip for ip in ips[i:i+15]]
        q = "ip[]=" + "&ip[]=".join(ip_to_check) + "&f=json"
        #req = urllib2.Request('http://www.stopforumspam.com/api', data)
        req = urllib2.Request('http://www.stopforumspam.com/api?%s' % q)
        print req.get_full_url()
        print req.get_data()
        try:
            response = urllib2.urlopen(req)
            data = json.loads(response.read())
            for rec in data['ip']:
                if rec['appears'] == 1:
                    _to_blacklist.append(rec['value'])
                    dprint("%s is *blacklisted*", rec['value'])
                else:
                   dprint("%s is NOT blacklisted", rec['value'])
        except urllib2.URLError, e:
            print e
            sys.exit()
    return _to_blacklist

if __name__ == "__main__":
    main()
    if DEBUG:
        sys.argv.append("-h")
    if TESTRUN:
        import doctest
        doctest.testmod()
    if PROFILE:
        import cProfile
        import pstats
        profile_filename = '_profile.txt'
        cProfile.run('main()', profile_filename)
        statsfile = open("profile_stats.txt", "wb")
        p = pstats.Stats(profile_filename, stream=statsfile)
        stats = p.strip_dirs().sort_stats('cumulative')
        stats.print_stats()
        statsfile.close()
        sys.exit(0)
    sys.exit()